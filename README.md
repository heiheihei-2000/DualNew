# DualR: åŒé‡å‰ªææœºåˆ¶çš„çŸ¥è¯†å›¾è°±é—®ç­”ç³»ç»Ÿ

## æœ€æ–°æ”¹è¿›

### åŠ¨æ€é—¨æ§èåˆæœºåˆ¶
æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªåŸºäºç¥ç»ç½‘ç»œçš„åŠ¨æ€é—¨æ§æœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°èåˆæ³¨æ„åŠ›åˆ†æ•°å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼š
- ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ›¿ä»£æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œæ›´å¥½åœ°æ•æ‰è¯­ä¹‰ç›¸å…³æ€§
- é€šè¿‡å¯å­¦ä¹ çš„é—¨æ§ç½‘ç»œè‡ªåŠ¨ç¡®å®šèåˆæƒé‡ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒæ•´
- æ ‡å‡†åŒ–å¤„ç†å¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§

### è‡ªé€‚åº”å‰ªæç­–ç•¥
- åŸºäºå®ä½“é‡è¦æ€§åŠ¨æ€è°ƒæ•´æ¯ä¸ªå®ä½“ä¿ç•™çš„è¾¹æ•°é‡
- é‡è¦å®ä½“ï¼ˆé«˜æ³¨æ„åŠ›åˆ†æ•°ï¼‰ä¿ç•™æ›´å¤šçš„è¿æ¥ï¼Œæé«˜å…³é”®ä¿¡æ¯çš„ä¿ç•™ç‡
- å‘½ä»¤è¡Œå‚æ•°`--top_k_edges`æ§åˆ¶åŸºç¡€è¾¹æ•°é‡ï¼ˆé»˜è®¤ä¸º3ï¼‰

### ç»Ÿä¸€è®¾å¤‡ç®¡ç†
- ä½¿ç”¨PyTorchæœ€ä½³å®è·µå¤„ç†è®¾å¤‡åˆ†é…ï¼š`next(model.parameters()).device`
- è‡ªåŠ¨æ£€æµ‹ç¼“å†²åŒºæ‰€åœ¨è®¾å¤‡å¹¶æŒ‰éœ€è½¬ç§»ï¼š`if tensor.device != current_device`
- åˆ›å»ºå¼ é‡æ—¶ç›´æ¥æŒ‡å®šè®¾å¤‡ï¼š`torch.zeros(..., device=current_device)`
- æ”¯æŒCPUå’Œä»»æ„GPUï¼š`--gpu -1`ä½¿ç”¨CPUï¼Œ`--gpu 0/1/2...`æŒ‡å®šGPUç¼–å·
- æ¨¡å‹åŠ è½½æ—¶è‡ªåŠ¨æ˜ å°„åˆ°å½“å‰è®¾å¤‡ï¼š`torch.load(file, map_location=current_device)`

## ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒæ¨¡å‹
```bash
# ä½¿ç”¨GPU 0è®­ç»ƒ
python explore/train.py --dataset webqsp --top_k_edges 3 --gpu 0

# ä½¿ç”¨CPUè®­ç»ƒ
python explore/train.py --dataset webqsp --top_k_edges 3 --gpu -1
```

### å‚æ•°è¯´æ˜
- `--dataset`: æ•°æ®é›†åç§°ï¼Œå¯é€‰å€¼åŒ…æ‹¬`MetaQA/1-hop`, `MetaQA/2-hop`, `MetaQA/3-hop`, `webqsp`, `CWQ`
- `--top_k_edges`: æ¯ä¸ªå®ä½“ä¿ç•™çš„åŸºç¡€è¾¹æ•°é‡ï¼Œé‡è¦å®ä½“ä¼šåŠ¨æ€å¢åŠ 
- `--gpu`: æŒ‡å®šä½¿ç”¨çš„GPUç¼–å·ï¼Œè®¾ä¸º-1åˆ™ä½¿ç”¨CPU

## ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œç›´æ¥å›ç­”

æˆ‘ä»¬å¢åŠ äº†ä¸€ä¸ªæ–°çš„åŠŸèƒ½ï¼Œå¯ä»¥åœ¨ç”Ÿæˆæ¨ç†è·¯å¾„åç›´æ¥å°†å…¶é€å…¥å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå›ç­”ï¼Œè€Œä¸æ˜¯ä¿å­˜åˆ°æ–‡ä»¶ä¸­ã€‚

### ä½¿ç”¨æ–¹æ³•

```bash
cd explore
python inference_with_llm.py --dataset webqsp --model_path WebCWQ_saved_model.pt --gpu 0
```

### ä¸»è¦å‚æ•°

- `--dataset`: æ•°æ®é›†åç§°ï¼Œå¯ä»¥æ˜¯ 'webqsp', 'CWQ', 'MetaQA/1-hop' ç­‰
- `--model_path`: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
- `--finetune`: æ˜¯å¦å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒ (å¯é€‰)
- `--gpu`: æŒ‡å®šGPU ID
- `--save_path`: å¦‚æœæŒ‡å®šï¼Œä¼šåŒæ—¶å°†è·¯å¾„ä¿å­˜åˆ°æ–‡ä»¶ (å¯é€‰)
- `--llm_model`: å¤§è¯­è¨€æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º "meta-llama/Llama-2-13b-chat-hf" (å¯é€‰)

### ç¤ºä¾‹

#### ç›´æ¥ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å›ç­”ï¼š

```bash
python inference_with_llm.py --dataset webqsp --model_path WebCWQ_saved_model.pt
```

#### ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å›ç­”å¹¶è¿›è¡Œå¾®è°ƒï¼š

```bash
python inference_with_llm.py --dataset webqsp --model_path WebCWQ_saved_model.pt --finetune
```

#### ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å›ç­”å¹¶åŒæ—¶ä¿å­˜è·¯å¾„ï¼š

```bash
python inference_with_llm.py --dataset webqsp --model_path WebCWQ_saved_model.pt --save_path webqsp-path.txt
```

## å¼•ç”¨
å¦‚æœæ‚¨ä½¿ç”¨äº†æœ¬ä»£ç ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š
```
@inproceedings{DualR,
  title={DualR: Dual Relation Pruning for Knowledge Graph Question Answering},
  author={},
  booktitle={},
  year={2023}
}
```

This is our Pytorch implementation for the paper (accepted by CPAL 2025):

> Liu, G., Zhang, Y., Li, Y., & Yao, Q. Dual Reasoning: A GNN-LLM Collaborative Framework for Knowledge Graph Question Answering. In *The Second Conference on Parsimony and Learning*

Link to paper: https://openreview.net/pdf?id=odnOkx8Qfj

## Instructions

A quick instruction is given for readers to reproduce the whole process.

## Environment Requirements

- torch == 2.5.1
- torch-cluster == 1.6.3
- torch-scatter == 2.1.2
- torchdrug == 0.2.1
- tokenizers == 0.20.3
- fairscale == 0.4.13
- fire == 0.5.0
- sentencepiece == 0.2.0

## Run the Codes

### Datasets

We follow the [NSM](https://github.com/RichardHGL/WSDM2021_NSM?tab=readme-ov-file) to preprocess the datasets.
You can download the datasets from NSM, and unzip them in the "data" folder.

### Experiments

#### Question and Relation Encoding

We use the Llama2-13B-chat as the encoder. Please download [it](https://github.com/meta-llama/llama) and put in the "llama" folder.

To get text embedding:

```
cd llama
bash getemb.sh
```

#### First-tier reasoning (knowledge exploration):

load pretrained model:

```
cd explore
python train.py  --dataset webqsp --load
```

or you can pretrain from scratch:

```
cd explore
python train.py  --dataset WebCWQ 
```

#### Using Distance-Attention Joint Pruning

Our model supports distance-attention joint pruning to achieve better balance between semantic relevance and structural importance. You can specify distance type and fusion strategy through command line arguments:

```
cd explore
# ä½¿ç”¨ä½™å¼¦è·ç¦»ä¸åŠ æƒæ±‚å’Œèåˆç­–ç•¥
python train.py --dataset webqsp --distance_type cosine --fusion_type weighted_sum

# ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ä¸é—¨æ§èåˆç­–ç•¥
python train.py --dataset webqsp --distance_type euclidean --fusion_type gating

# ä½¿ç”¨æ›¼å“ˆé¡¿è·ç¦»ä¸ä¹˜æ€§èåˆç­–ç•¥
python train.py --dataset webqsp --distance_type manhattan --fusion_type multiplicative
```

Available options:
- `distance_type`: [cosine, euclidean, manhattan]
- `fusion_type`: [weighted_sum, gating, multiplicative]

#### Using Improved Distance-Attention Joint Pruning with Dynamic Weight Adjustment

We have implemented an improved version of the distance-attention joint pruning mechanism that dynamically adjusts weights based on the model's performance during training. This approach optimizes the balance between attention and distance scores automatically.

```
cd explore
# ä½¿ç”¨é»˜è®¤å‚æ•°çš„æ”¹è¿›å‰ªææœºåˆ¶
python train.py --dataset webqsp --distance_type euclidean --top_k_edges 3 --adjustment_rate 0.02 --initial_attn_weight 0.5 --initial_dist_weight 0.5

# è°ƒæ•´ä¿ç•™çš„è¾¹æ•°é‡
python train.py --dataset webqsp --distance_type euclidean --top_k_edges 5 --adjustment_rate 0.02

# å¢å¤§æƒé‡è°ƒæ•´å¹…åº¦ï¼ŒåŠ å¿«æƒé‡æœç´¢
python train.py --dataset webqsp --distance_type euclidean --adjustment_rate 0.05 

# è°ƒæ•´åˆå§‹æƒé‡æ¯”ä¾‹ï¼Œæ›´åå‘è·ç¦»åˆ†æ•°
python train.py --dataset webqsp --initial_attn_weight 0.3 --initial_dist_weight 0.7
```

Available parameters:
- `distance_type`: Currently supports `euclidean` distance measurement
- `top_k_edges`: Number of edges to keep per entity (default: 3)
- `adjustment_rate`: The rate at which weights are adjusted during training (default: 0.02)
- `initial_attn_weight`: Initial weight for attention scores (default: 0.5)
- `initial_dist_weight`: Initial weight for distance scores (default: 0.5)

During training, the model will:
1. Evaluate H@1 performance after each epoch
2. Adjust attention and distance weights based on performance changes
3. Record the best-performing weight configuration
4. Save these optimal weights alongside the model

#### Testing and Visualizing Pruning Effects

We provide a testing script to evaluate and visualize the effects of different pruning strategies:

```
cd explore
# æµ‹è¯•å•ä¸ªå‰ªæç­–ç•¥çš„æ•ˆæœ
python test_dual_pruning.py --dataset webqsp --model_path WebCWQ_saved_model.pt --distance_type cosine --fusion_type weighted_sum

# æ¯”è¾ƒæ‰€æœ‰è·ç¦»-èåˆç­–ç•¥ç»„åˆçš„æ•ˆæœ
python test_dual_pruning.py --dataset webqsp --model_path WebCWQ_saved_model.pt --compare_all
```

This will generate visualizations showing:
- Node and edge count changes before and after pruning
- Distribution of attention scores and distance scores
- Pruning rates for each layer
- Comparative analysis of different pruning strategies

#### Second-tier reasoning (answer determination):

use Llama2-13B-chat:

```
cd llama
bash  chat13.sh
```

use ChatGPT:

```
cd llama
python gpt.py --dataset webqsp
```

## DualR-Enhanced: åŒé‡æ¨ç†å¢å¼ºæ¡†æ¶

æˆ‘ä»¬å®ç°äº†å…¨æ–°çš„ **DualR-Enhanced** æ¶æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªå››é˜¶æ®µçš„åŒé‡æ¨ç†å¢å¼ºçŸ¥è¯†å›¾è°±é—®ç­”æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹æ ¸å¿ƒåˆ›æ–°ï¼š

### ğŸš€ æ ¸å¿ƒåˆ›æ–°ç‚¹

1. **å¤šæ¨¡æ€å›¾è¡¨ç¤ºèåˆ** (Multi-Modal Graph Representation Fusion)
2. **æ¸è¿›å¼æ³¨æ„åŠ›å‰ªæ** (Progressive Attention Pruning)  
3. **ç»“æ„åŒ–å¤šé€‰æç¤ºç”Ÿæˆ** (Structured Multi-Choice Prompt Generation)
4. **è‡ªé€‚åº”å›¾-æ–‡æœ¬å¯¹é½** (Adaptive Graph-Text Alignment)

### ğŸ—ï¸ å››é˜¶æ®µæ¶æ„æµç¨‹

#### é˜¶æ®µ1: æ¸è¿›å¼å›¾æ¢ç´¢ (Progressive Graph Exploration)
- **åŠ¨æ€å‰ªæç­–ç•¥**: åŸºäºæ³¨æ„åŠ›åˆ†æ•°å’Œè¯­ä¹‰ç›¸ä¼¼åº¦çš„è‡ªé€‚åº”æƒé‡èåˆ
- **å¤šå±‚çº§æ‰©å±•**: ç¬¬1è·³ä¿ç•™top-5è¾¹ï¼Œç¬¬2è·³ä¿ç•™top-4è¾¹ï¼Œç¬¬3è·³ä¿ç•™top-3è¾¹
- **æ™ºèƒ½èŠ‚ç‚¹é€‰æ‹©**: è‡ªåŠ¨æ”¶é›†æ‰©å±•åçš„æ‰€æœ‰èŠ‚ç‚¹ç”¨äºåç»­å¤„ç†

#### é˜¶æ®µ2: å¤šæ¨¡æ€è¡¨ç¤ºç”Ÿæˆ (Multi-Modal Representation Generation)
- **ç»“æ„åŒ–å›¾è¡¨ç¤º**: é€šè¿‡æ³¨æ„åŠ›æ± åŒ–ç”Ÿæˆå…¨å±€å›¾è¡¨ç¤ºï¼ŒæŠ•å½±åˆ°LLMè¯­ä¹‰ç©ºé—´
- **æ–‡æœ¬åŒ–å›¾è¡¨ç¤º**: å°†æ¨ç†è·¯å¾„è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°
- **å¤šé€‰æç¤ºå¢å¼º**: è‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–çš„3é€‰é¡¹æç¤ºï¼ŒåŒ…å«ç½®ä¿¡åº¦å’Œæ¨ç†è·¯å¾„

#### é˜¶æ®µ3: è‡ªé€‚åº”èåˆä¸ç”Ÿæˆ (Adaptive Fusion and Generation)
- **æ³¨æ„åŠ›é—¨æ§èåˆ**: åŠ¨æ€å¹³è¡¡å›¾ç»“æ„ä¿¡æ¯å’Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯
- **è½¯æç¤ºæ„å»º**: ç”Ÿæˆå›¾ç»“æ„æç¤ºå’Œæ–‡æœ¬è¯­ä¹‰æç¤ºçš„è”åˆè¡¨ç¤º
- **ç”Ÿæˆå‡†å¤‡**: ä¸ºLLMæ„å»ºæœ€ä¼˜çš„è¾“å…¥åºåˆ—æ ¼å¼

#### é˜¶æ®µ4: å¯¹æ¯”å­¦ä¹ ä¸ä¼˜åŒ– (Contrastive Learning and Optimization)
- **å¤šä»»åŠ¡æŸå¤±å‡½æ•°**: å›¾é‡æ„æŸå¤± + æ–‡æœ¬ç”ŸæˆæŸå¤± + å¯¹æ¯”å­¦ä¹ æŸå¤±
- **è‡ªé€‚åº”æƒé‡è°ƒæ•´**: æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´å„æŸå¤±ç»„ä»¶æƒé‡
- **æ€§èƒ½è¯„ä¼°**: ç­”æ¡ˆå‡†ç¡®ç‡ã€æ¨ç†è´¨é‡ã€å¤šé€‰ä¸€è‡´æ€§çš„ç»¼åˆè¯„ä¼°

### ğŸ”§ è”åˆè®­ç»ƒæ¨¡å¼

æˆ‘ä»¬æä¾›äº†ä¸‰ç§è®­ç»ƒæ–¹æ³•ï¼š

#### 1. äº¤æ›¿è®­ç»ƒæ¨¡å¼ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
åœ¨æ¯ä¸ªepochä¸­ï¼Œå…ˆè®­ç»ƒGNNæ¨¡å‹ï¼Œç„¶åä½¿ç”¨GNNçš„è¾“å‡ºè®­ç»ƒLLMã€‚

#### 2. ç»Ÿä¸€è”åˆè®­ç»ƒæ¨¡å¼ï¼ˆæ”¹è¿›æ–¹å¼ï¼‰
åœ¨åŒä¸€ä¸ªå‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼ŒGNNè¾“å‡ºç›´æ¥è¿æ¥åˆ°LLMè¾“å…¥ï¼Œé€šè¿‡è”åˆæŸå¤±å‡½æ•°åŒæ—¶æ›´æ–°å‚æ•°ã€‚

#### 3. DualR-Enhancedæ¨¡å¼ï¼ˆæœ€æ–°æ¶æ„ï¼‰
å®ç°å®Œæ•´çš„å››é˜¶æ®µåŒé‡æ¨ç†å¢å¼ºæ¡†æ¶ï¼Œå…·æœ‰å¤šæ¨¡æ€èåˆã€è‡ªé€‚åº”æƒé‡å’Œå¯¹æ¯”å­¦ä¹ ã€‚

### ğŸ“š ä½¿ç”¨æ–¹æ³•

#### 1. äº¤æ›¿è®­ç»ƒæ¨¡å¼ï¼š
```bash
cd explore/training
python joint_training.py --dataset webqsp --epochs 20 --K 100 --gpu 0 --output_dir ./joint_model
```

#### 2. ç»Ÿä¸€è”åˆè®­ç»ƒæ¨¡å¼ï¼š
```bash
cd explore/training
python joint_training.py --dataset webqsp --epochs 20 --K 100 --gpu 0 --output_dir ./joint_unified_model --unified_training --gnn_lr 1e-5 --llm_lr 1e-4 --gnn_loss_weight 0.5 --llm_loss_weight 0.5
```

#### 3. â­ DualR-Enhancedæ¨¡å¼ï¼ˆæ¨èï¼‰ï¼š
```bash
cd explore/training
python joint_training_unified.py --dataset webqsp --epochs 15 --K 100 --gpu 0 --output_dir ./dualr_enhanced_model --gnn_lr 1e-5 --llm_lr 1e-4
```

#### DualR-Enhanced é«˜çº§é…ç½®ï¼š
```bash
# ä½¿ç”¨é¢„è®­ç»ƒGNNæ¨¡å‹
python joint_training_unified.py --dataset webqsp --epochs 15 --load_gnn --gnn_model_path WebCWQ_saved_model.pt --output_dir ./dualr_enhanced_model

# è‡ªå®šä¹‰æŸå¤±æƒé‡
python joint_training_unified.py --dataset webqsp --epochs 15 --gnn_loss_weight 0.3 --llm_loss_weight 0.5 --output_dir ./dualr_enhanced_model

# è°ƒæ•´æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
python joint_training_unified.py --dataset webqsp --epochs 15 --gradient_accumulation_steps 8 --output_dir ./dualr_enhanced_model
```

### ğŸ“‹ ä¸»è¦å‚æ•°è¯´æ˜

#### ğŸ”§ é€šç”¨å‚æ•°ï¼š
- `--dataset`: æ•°æ®é›†åç§° ('webqsp', 'CWQ', 'MetaQA/1-hop', 'MetaQA/2-hop', 'MetaQA/3-hop')
- `--epochs`: è®­ç»ƒè½®æ•°ï¼Œæ¨è15-20è½®
- `--K`: å›¾æ¢ç´¢ä¸­æ¯ä¸ªå®ä½“çš„é‚»å±…é‡‡æ ·æ•°é‡ï¼Œé»˜è®¤100
- `--gpu`: GPUè®¾å¤‡IDï¼Œ-1è¡¨ç¤ºä½¿ç”¨CPU
- `--llm_model`: LLMæ¨¡å‹åç§°ï¼Œé»˜è®¤"meta-llama/Llama-2-7b-hf"
- `--eval_every`: è¯„ä¼°é¢‘ç‡ï¼Œé»˜è®¤æ¯1ä¸ªepochè¯„ä¼°ä¸€æ¬¡
- `--output_dir`: æ¨¡å‹ä¿å­˜ç›®å½•

#### ğŸ¯ DualR-Enhanced ä¸“ç”¨å‚æ•°ï¼š
- `--gnn_lr`: GNNå­¦ä¹ ç‡ï¼Œæ¨è1e-5
- `--llm_lr`: LLMå­¦ä¹ ç‡ï¼Œæ¨è1e-4  
- `--gnn_loss_weight`: å›¾é‡æ„æŸå¤±æƒé‡ï¼Œé»˜è®¤0.3
- `--llm_loss_weight`: æ–‡æœ¬ç”ŸæˆæŸå¤±æƒé‡ï¼Œé»˜è®¤0.5
- `--gradient_accumulation_steps`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œé»˜è®¤4
- `--load_gnn`: æ˜¯å¦åŠ è½½é¢„è®­ç»ƒGNNæ¨¡å‹
- `--gnn_model_path`: é¢„è®­ç»ƒGNNæ¨¡å‹è·¯å¾„

#### ğŸ“Š ä¼ ç»Ÿæ¨¡å¼å‚æ•°ï¼š
- `--unified_training`: å¯ç”¨ç»Ÿä¸€è”åˆè®­ç»ƒæ¨¡å¼  
- `--llm_train_freq`: äº¤æ›¿æ¨¡å¼ä¸­LLMè®­ç»ƒé¢‘ç‡

### ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

#### â­ DualR-Enhanced æ¨¡å¼ç¤ºä¾‹ï¼š

##### ğŸš€ å¿«é€Ÿå¼€å§‹ (æ¨èæ–°ç”¨æˆ·):
```bash
cd explore/training
python joint_training_unified.py --dataset webqsp --epochs 15 --gpu 0
```

##### ğŸ¯ é«˜æ€§èƒ½é…ç½® (æ¨èæœ€ä½³æ•ˆæœ):
```bash
python joint_training_unified.py --dataset webqsp --epochs 20 --K 100 --gnn_lr 1e-5 --llm_lr 1e-4 --gradient_accumulation_steps 4 --gpu 0
```

##### ğŸ”„ ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹:
```bash
python joint_training_unified.py --dataset webqsp --epochs 10 --load_gnn --gnn_model_path WebCWQ_saved_model.pt --gpu 0
```

##### âš™ï¸ è‡ªå®šä¹‰æŸå¤±æƒé‡ (å®éªŒæ€§):
```bash
python joint_training_unified.py --dataset webqsp --epochs 15 --gnn_loss_weight 0.2 --llm_loss_weight 0.6 --gpu 0
```

#### ğŸ“Š ä¼ ç»Ÿæ¨¡å¼ç¤ºä¾‹ï¼š

##### äº¤æ›¿è®­ç»ƒæ¨¡å¼:
```bash
python joint_training.py --dataset webqsp --epochs 20 --K 100 --gpu 0
```

##### ç»Ÿä¸€è”åˆè®­ç»ƒæ¨¡å¼:
```bash
python joint_training.py --dataset webqsp --epochs 15 --K 100 --unified_training --gnn_lr 2e-5 --llm_lr 5e-5 --gpu 0
```

### ğŸ” è®­ç»ƒæ¨¡å¼å¯¹æ¯”

| ç‰¹æ€§ | äº¤æ›¿è®­ç»ƒæ¨¡å¼ | ç»Ÿä¸€è”åˆè®­ç»ƒæ¨¡å¼ | **â­ DualR-Enhancedæ¨¡å¼** |
|------|-------------|-----------------|-------------------------|
| **æ¶æ„å¤æ‚åº¦** | ç®€å• | ä¸­ç­‰ | **é«˜çº§** |
| **å‚æ•°æ›´æ–°** | åˆ†åˆ«æ›´æ–° | åŒæ—¶æ›´æ–° | **å››é˜¶æ®µè”åˆæ›´æ–°** |
| **æ¢¯åº¦ä¼ æ’­** | åˆ†ç¦»æ¢¯åº¦æµ | ç»Ÿä¸€æ¢¯åº¦æµ | **å¤šæ¨¡æ€æ¢¯åº¦æµ** |
| **æŸå¤±å‡½æ•°** | å•ä¸€æŸå¤± | åŠ æƒæŸå¤± | **å¤šä»»åŠ¡æŸå¤± + å¯¹æ¯”å­¦ä¹ ** |
| **è¡¨ç¤ºå­¦ä¹ ** | ç‹¬ç«‹è¡¨ç¤º | ç®€å•èåˆ | **ç»“æ„åŒ– + æ–‡æœ¬åŒ–åŒé‡è¡¨ç¤º** |
| **æ³¨æ„åŠ›æœºåˆ¶** | åŸºç¡€æ³¨æ„åŠ› | ç®€å•æ³¨æ„åŠ› | **æ¸è¿›å¼æ³¨æ„åŠ›å‰ªæ** |
| **æç¤ºå·¥ç¨‹** | æ—  | åŸºç¡€æç¤º | **ç»“æ„åŒ–3é€‰é¡¹æç¤º** |
| **å†…å­˜ä½¿ç”¨** | ä½ | ä¸­ç­‰ | **è¾ƒé«˜** |
| **è®­ç»ƒé€Ÿåº¦** | å¿« | ä¸­ç­‰ | **è¾ƒæ…¢** |
| **é¢„æœŸæ•ˆæœ** | ç¨³å®š | æ›´å¥½èåˆ | **æœ€ä½³æ€§èƒ½** |
| **é€‚ç”¨åœºæ™¯** | å¿«é€Ÿå®éªŒ | æ€§èƒ½æå‡ | **è¿½æ±‚SOTAæ•ˆæœ** |

### ğŸ¯ æ¨èä½¿ç”¨ç­–ç•¥

- **ğŸš€ å¿«é€ŸéªŒè¯**: ä½¿ç”¨äº¤æ›¿è®­ç»ƒæ¨¡å¼è¿›è¡Œæ¦‚å¿µéªŒè¯
- **âš–ï¸ å¹³è¡¡é€‰æ‹©**: ä½¿ç”¨ç»Ÿä¸€è”åˆè®­ç»ƒæ¨¡å¼è·å¾—æ›´å¥½çš„èåˆæ•ˆæœ  
- **ğŸ† æœ€ä½³æ€§èƒ½**: ä½¿ç”¨ **DualR-Enhanced æ¨¡å¼** è¿½æ±‚æœ€é«˜å‡†ç¡®ç‡å’Œæ¨ç†è´¨é‡

### ğŸ“ˆ é¢„æœŸæ€§èƒ½æå‡

DualR-Enhanced æ¨¡å¼ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•é¢„æœŸå¯è·å¾—ï¼š
- **H@1å‡†ç¡®ç‡**: æå‡5-8%
- **æ¨ç†è´¨é‡**: æ˜¾è‘—æå‡ (é€šè¿‡ç»“æ„åŒ–å¤šé€‰æç¤º)
- **è®­ç»ƒç¨³å®šæ€§**: æ”¹å–„ (è‡ªé€‚åº”æƒé‡è°ƒæ•´)
- **æ¨¡å‹æ³›åŒ–**: å¢å¼º (å¯¹æ¯”å­¦ä¹ æœºåˆ¶)

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œè®­ç»ƒç»“æŸåä¿å­˜æœ€ç»ˆæ¨¡å‹ã€‚æ‰€æœ‰æ¨¡å¼éƒ½æ”¯æŒä»é¢„è®­ç»ƒæ¨¡å‹ç»§ç»­è®­ç»ƒã€‚
